# ðŸš€ Transformer Architecture

This repository contains a modular implementation of a **Transformer model built entirely from scratch using NumPy**, without using PyTorch or TensorFlow.  
It also includes training notebooks, Word2Vec-based embeddings, and utilities for low-level neuron analysis and debugging.

## ðŸ”¶ Components Overview

### 1. `src/transformer.py`
Implements the full Transformer architecture based on the *Attention Is All You Need* paper.

#### âœ” Core Modules
- **Self Attention**
- **Scaled Dot-Product Attention**
- **Feed-Forward Networks (FFN)**
- **Residual Connections + LayerNorm**
- **Positional Encoding**
- **Encoder Layer**
- **Decoder Layer**
- **Masked (causal) attention for decoding**
- **Cross-attention** between encoder â†’ decoder

Supports:
- Batching  
- Sequence-level attention  
- Word2Vec embeddings as token vectors  

---

### 2. `src/MPNeuronInfo.py`
Contains fundamental neural components implemented from scratch:

#### âœ” Layers
- `Layer_Dense`
- `Activation_ReLU`
- `Activation_Softmax`

#### âœ” Loss Function
- `Loss_CrossCategoricalEntropy`

#### âœ” Optimizer
- `OptimizerAdam` (with momentum, RMS, and bias correction)

These mimic deep learning library internals but are written manually for transparency.

---

### 3. Tokenization & Embeddings
The project uses:

- `nltk.word_tokenize` for tokenization  
- `gensim.Word2Vec` for dense vector embeddings  

Workflow:
1. Tokenize English/Spanish sentences  
2. Convert tokens â†’ vectors via Word2Vec  
3. Pass sequence embeddings â†’ Transformer  

---

### 4. Training Notebook (`notebooks/transformer_training.ipynb`)

Shows complete flow:

#### âœ” Data Preprocessing
- Tokenization  
- Vocabulary mapping  
- Embedding lookup  
- Padding & batching  

#### âœ” Training Loop
- Forward pass  
- Loss computation  
- Backpropagation  
- Parameter updates (Adam)  
- Logging loss curves  

#### âœ” Inference Logic
- Start with `<SOS>` token  
- Autoregressive decoding  
- Add positional encodings each step  
- Use encoder output for all decoding steps  

---

## ðŸ›  Setup

```bash
python -m venv .venv
source .venv/bin/activate     # On Windows: .venv\Scripts\activate
pip install -r requirements.txt

