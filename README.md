# Transformer-Architechture
Built a basic Transformer architechture from scratch for an in-depth study of the underlying mechanics of LLMs.

Tested the working of the model for a standard use case of Spanish to English Translation.
Defined a custom dictionary using a paragraph from a random english passage along with its spanish translation.
Was able to construct the key components of the Transformer using basic principles of OOPS (in Python):
-- Encoder
-- Decoder
-- Self-Attention and Cross-Attention mechanisms
-- Standard positional encoding
-- Used Word2vec for word embeddings
-- Look-ahead Masking of the input values

The model lacks :
-- Multi-Head meachanism
-- Dropout Layers

Tested tha model on multiple translations picked from the paragraph.
